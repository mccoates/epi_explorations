---
title: "Comparison of Methods for Analyzing RCT Data Regarding Baseline Treatment Group Differences"
format: html
editor: visual
---

### Motivation:

-   Seeing multiple approaches for analyzing fairly basic structures of RCT data
-   Reading that some strategies better adjust for **regression to the mean** because of baseline differences in treatment and control group (Twisk, 2018)
-   Wanting to understand the DAG structures that illustrate the "difference in differences" and "longitudinal analysis of covariance" approaches/implied data generating processes
-   Talking to professor in Bayesian analysis class who strongly preferred a difference in differences approach with an individual-level random effect

```{r}
#| echo: false
#| output: false
# setup
rm(list=ls())
library(data.table)
library(ggplot2)
library(ggdag)
library(dagitty)
library(gridExtra)

## define useful functions
expit <- function(x) {exp(x)/(1+exp(x))}
source("C:/Users/MattC/Documents/repos/epi_explorations/RCT_options_functions.R")

## small number to have more likelihood for imbalance
N <- 30
niter <- 1000
```

## Analysis

### Overview

(1) Simulated RCT data using multiple data generating processes for comparison.

(2) Fit generalized linear models with varied specifications to estimate the effect of treatment on the outcome of interest, comparing error over many simulations, as well as examining relationship between error and the baseline imbalance in the outcome.

### Data Generating Processes

I tested multiple data generating processes to understand in which contexts different modeling strategies led to different results. There were two different broad categories under which the data generating processes fit based on how the outcome was generated, which I call (1) conditional on baseline and (2) repeated measures.

#### DGP 1: Conditional on Baseline

These DGPs use the following variables:

$X$ = Treatment Status from RCT

$Y_0$ = Outcome at Baseline (Pre-treatment)

$Y_1$ = Outcome at Follow-up (Post-treatment)

DGPs:

(1A) $Y_{1}=\alpha_0 + \alpha_X X + \alpha_{Y_0} Y_{0} + \epsilon$

(1B) $Y_{1}=\beta_0 + \beta_X X + \beta_{Y_0} Y_{0} + \beta_{X*Y_0} X*Y_{0} + \epsilon$

#### DGP 2: Repeated Measures

These DGPs use the following variables:

$Arm$ = Indicator of Treatment Study Arm

$Post$ = Indicator of Time (0=Pre-Intervention, 1=Post-Intervention)

$\phi$ = Individual-Specific Random Effect

$Y_t$ = Outcome at Time $t$ (where $t$ is reflected in $Post$)

(2A) $Y_t = \gamma_0 + \gamma_{Arm} Arm + \gamma_{Post} Post + \gamma_{Arm*Post} Arm*Post + \epsilon$

(2B) $Y_t = \delta_0 + \delta_{Arm} Arm + \delta_{Post} Post + \delta_{Arm*Post} Arm*Post + \phi_i + \epsilon$

#### QUESTION: How to represent random effect in DAG? U_y separated out? Or two nodes--one for Y_1 and one for Y_0? And confirm how to simulate with RE correctly.

#### Could RE also be in DGP 1 by simulating some variance of Y

#### To Do: Redo figures in Causal Fusion, looks nicer


#### Question: in DGP 2a/2b, how to deal with potentially different variances in Y_0 and Y_1 when we're just generating Y_t?


```{r}
#| echo: false

dag1 <- dagitty::dagitty(
'dag {
  "X*Y0" [pos="0.1,0.357"]
  X [exposure,pos="-0.239,-0.459"]
  Y0 [pos="-0.129,1.076"]
  Y1 [outcome,pos="0.752,0.464"]
  X -> "X*Y0"
  X -> Y1
  Y0 -> "X*Y0"
  Y0 -> Y1
}'
)

dag2 <- dagitty::dagitty(
'dag {
  "X*Y0" [pos="0.1,0.357"]
  X [exposure,pos="-0.239,-0.459"]
  Y0 [pos="-0.129,1.076"]
  Y1 [outcome,pos="0.752,0.464"]
  "X*Y0" -> Y1
  X -> "X*Y0"
  X -> Y1
  Y0 -> "X*Y0"
  Y0 -> Y1
}'
)

dag3 <- dagitty::dagitty(
'dag {
  "Arm*Post" [exposure,pos="-0.329,0.265"]
  Arm [pos="-1.761,0.654"]
  Post [pos="-0.424,-0.489"]
  Yt [outcome,pos="1.198,0.644"]
  "Arm*Post" -> Yt
  Arm -> "Arm*Post"
  Arm -> Yt
  Post -> "Arm*Post"
  Post -> Yt
}'
)


dag4 <- dagitty::dagitty(
'dag {
  "Arm*Post" [exposure,pos="-0.329,0.265"]
  "Φ" [pos="1.053,-0.210"]
  Arm [pos="-1.761,0.654"]
  Post [pos="-0.424,-0.489"]
  Yt [outcome,pos="1.198,0.644"]
  "Arm*Post" -> Yt
  "Φ" -> Yt
  Arm -> "Arm*Post"
  Arm -> Yt
  Post -> "Arm*Post"
  Post -> Yt
}'
)


gg1 <- ggdag(dag1, layout = "circle") + theme_dag_blank() + annotate("text",x=.45,y=.48,label="Effect = 0") + ggtitle("DGP 1A: Conditional on Baseline,\nNo Interaction (on Additive Scale)")

gg2 <- ggdag(dag2, layout = "circle") + theme_dag_blank() + 
  annotate("text",x=.45,y=.48,label="Effect =/= 0") + ggtitle("DGP 1B: Conditional on Baseline,\nInteraction (on Additive Scale)")

gg3 <- ggdag(dag3, layout = "circle") + theme_dag_blank() + 
  ggtitle("DGP 2A: Repeated Measures\n(no Random Effect)")

gg4 <- ggdag(dag4, layout = "circle") + theme_dag_blank() + 
  ggtitle("DGP 2B: Repeated Measures\n(with Random Effect)")

grid.arrange(gg1, gg2, nrow = 1)

grid.arrange(gg3, gg4, nrow = 1)

#ggplot(dag)


```

I used these DGPs to simulate each of these 4 types of datasets with varying sample sizes (30, 300, 3000), each 1,000 times.

### Models Fit

I fit models specified like each of the DGPs above on each of the datasets simulated as specified in the DGPs above (4 models fit on each of 4 datasets for 16 combinations). I compared the following analytic approaches (names from Twisk et al., 2018):

(1) "Longitudinal Analysis of Covariance"

(1A) $Y_{1}=\alpha_0 + \alpha_X X + \alpha_{Y_0} Y_{0} + \epsilon$

(1B) $Y_{1}=\beta_0 + \beta_X X + \beta_{Y_0} Y_{0} + \beta_{X*Y_0} X*Y_{0} + \epsilon$


(2) "Repeated Measures Analysis" (Difference in Differences)

(2A) $Y_t = \gamma_0 + \gamma_{Arm} Arm + \gamma_{Post} Post + \gamma_{Arm*Post} Arm*Post + \epsilon$

(2B) $Y_t = \delta_0 + \delta_{Arm} Arm + \delta_{Post} Post + \delta_{Arm*Post} Arm*Post + \phi_i + \epsilon$


### Results

```{r}
#| echo: false

bd <- data.table(data.frame(d1a=c(),d1b=c(),d2a=c(),d2b=c()))
res <- array(data=rep(NA,4*4*N),dim=c(4,4,niter))
d <- list() ## data
for (i in 1:niter) {
  #cat(paste0(i)); flush.console()
 
  ## Data generation
  d[["1a"]] <- dgp1a(N=N) ## DGP 1a
  d[["1b"]] <- dgp1b(N=N) ## DGP 1b
  d[["2a"]] <- dgp2a(N=N) ## DGP 2a
  d[["2b"]] <- dgp2b(N=N) ## DGP 2b

  ## find imbalance in baseline values for each of
  ## the four simulated datasets
  bd <- rbind(bd,imbalance(d),use.names=F)
  
  ## regression models
  m1aform <- "Y1 ~ Y0 + E"
  m1bform <- "Y1 ~ Y0 + E + Y0*E"
  m2aform <- "Y ~ arm + post + arm*post"
  m2bform <- "Y ~ arm + post + arm*post + (1|id)"
  formlist <- list(m1aform,m1bform,m2aform,m2bform)
  names(formlist) <- c("d1a","d1b","d2a","d2b")
  
  ## run each model on each dataset with key outputs extracted
  res[,,i] <- run_model(forms=formlist,d=d)

}

means <- apply(res,MARGIN=c(1,2),mean)
sds <- apply(res,MARGIN=c(1,2),sd)

mean_bd <- apply(as.matrix(bd),MARGIN=2,mean)

summary(lm((res[1,1,]+2) ~ unlist(bd[,1])))
summary(lm((res[1,2,]+2) ~ unlist(bd[,1])))
summary(lm((res[1,3,]+2) ~ unlist(bd[,1])))
summary(lm((res[1,4,]+2) ~ unlist(bd[,1])))

summary(lm(abs(res[2,1,]+1) ~ unlist(bd[,2])))
summary(lm(abs(res[2,2,]+2) ~ unlist(bd[,2])))
summary(lm(abs(res[2,3,]+1) ~ unlist(bd[,2])))
summary(lm(abs(res[2,4,]+1) ~ unlist(bd[,2])))

summary(lm((res[2,1,]+1) ~ unlist(bd[,2])))
summary(lm((res[2,2,]+2) ~ unlist(bd[,2])))
summary(lm((res[2,3,]+1) ~ unlist(bd[,2])))
summary(lm((res[2,4,]+1) ~ unlist(bd[,2])))

summary(lm(abs(res[3,1,]+2) ~ unlist(bd[,2])))
summary(lm(abs(res[3,2,]+2) ~ unlist(bd[,2])))
summary(lm(abs(res[3,3,]+2) ~ unlist(bd[,2])))
summary(lm(abs(res[3,4,]+2) ~ unlist(bd[,2])))

tmp <- data.frame(resid=res[2,4,]+1,b_diff=unlist(bd[,2]))

gg <- ggplot(data=tmp,aes(x=b_diff,y=resid)) + geom_point() + theme_bw() + geom_smooth()
print(gg)


# m1diff <- abs(m1+2)
# m2diff <- abs(m2+2)
# mean(m1)
# mean(m2)
# summary(lm(m1diff ~ bd))
# summary(lm(m2diff ~ bd))
# mean(bd)

```

Notes:

Note to look back: https://diff.healthpolicydatascience.org/#regression

not the same but maybe related https://www.sciencedirect.com/science/article/abs/pii/S0304407614002437?via%3Dihub https://onlinelibrary.wiley.com/doi/10.1111/1475-6773.13017 https://onlinelibrary.wiley.com/doi/full/10.1111/1475-6773.13015 https://pubmed.ncbi.nlm.nih.gov/33978956/

Note: ATT/ATE? IS THE FACT THAT IT IS ATT SOMEHOW ENCODED IN GRAPH? HOW CAN WE EXPLAIN THE TRANSFORMATION OF THE GRAPH?

Note: Do with continuous X too?
